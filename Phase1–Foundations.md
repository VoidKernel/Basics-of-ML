Phase 1 â€“ Foundations

Python (numpy, pandas)

statistics basics

scikit-learn

data preprocessing

1. NumPy (numbers, fast, no feelings)(brain)

Used for:

Arrays

Vectors & matrices

Linear algebra

ML math

Speed

2. Pandas (tables, CSVs, emotional damage)(hands)

Used for:

CSV / Excel

Cleaning data

Filtering

Feature engineering

Data analysis


statistics basics

1. Types of Data

1.1 Numerical (Quantitative)

Continuous: can take any value (height, weight, temperature)

Discrete: countable (number of students, cars)

1.2 Categorical (Qualitative)

Nominal: names, labels (red, blue, cat, dog)

Ordinal: ordered categories (small, medium, large; grades A, B, C)

2. Measures of Central Tendency

These describe â€œtypicalâ€ value.

Measure	Formula / Idea
Mean (average)	
âˆ‘
ğ‘¥
ğ‘–
ğ‘›
n
âˆ‘x
i
	â€‹

	â€‹


Median	Middle value after sorting
Mode	Most frequent value

Example: [1, 2, 2, 3, 4]

Mean = 2.4

Median = 2

Mode = 2

3. Measures of Spread (Dispersion)

These tell you how scattered your data is.

Measure	Formula / Idea
Range	Max â€“ Min
Variance	
âˆ‘
(
ğ‘¥
ğ‘–
âˆ’
ğ‘¥
Ë‰
)
2
ğ‘›
n
âˆ‘(x
i
	â€‹

âˆ’
x
Ë‰
)
2
	â€‹


Standard Deviation (SD)	
ğ‘‰
ğ‘
ğ‘Ÿ
ğ‘–
ğ‘
ğ‘›
ğ‘
ğ‘’
Variance
	â€‹


Interquartile Range (IQR)	Q3 â€“ Q1

High SD â†’ data all over the place. Low SD â†’ data hugs the mean.

4. Probability Basics

Probability of event A:

ğ‘ƒ
(
ğ´
)
=
Number of favorable outcomes
Total outcomes
P(A)=
Total outcomes
Number of favorable outcomes
	â€‹


Example: roll a die â†’ P(get 3) = 1/6

5. Common Distributions

Normal (Gaussian): bell curve, mean = median = mode

Uniform: all outcomes equally likely

Binomial: yes/no repeated experiments (coin toss, success/failure)

Poisson: count of events in fixed interval (emails per hour)

ML loves the normal distribution. Most algorithms assume it somewhere.

6. Correlation

Measures relationship between two variables:

ğ‘Ÿ
=
Cov
(
ğ‘‹
,
ğ‘Œ
)
ğœ
ğ‘‹
ğœ
ğ‘Œ
r=
Ïƒ
X
	â€‹

Ïƒ
Y
	â€‹

Cov(X,Y)
	â€‹


r = 1 â†’ perfect positive

r = -1 â†’ perfect negative

r â‰ˆ 0 â†’ no linear relationship

Pearson correlation is what ML people usually mean.

7. Covariance
ğ¶
ğ‘œ
ğ‘£
(
ğ‘‹
,
ğ‘Œ
)
=
âˆ‘
(
ğ‘‹
ğ‘–
âˆ’
ğ‘‹
Ë‰
)
(
ğ‘Œ
ğ‘–
âˆ’
ğ‘Œ
Ë‰
)
ğ‘›
Cov(X,Y)=
n
âˆ‘(X
i
	â€‹

âˆ’
X
Ë‰
)(Y
i
	â€‹

âˆ’
Y
Ë‰
)
	â€‹


Positive â†’ Xâ†‘ then Yâ†‘

Negative â†’ Xâ†‘ then Yâ†“

Magnitude is hard to interpret â†’ use correlation

8. Skewness & Kurtosis

Skewness â†’ asymmetry of data

Kurtosis â†’ â€œpeakednessâ€ or tail heaviness

Useful to know if your data is weird before feeding it to ML.

9. Summary Statistics
import pandas as pd

df = pd.DataFrame({"Score": [10,20,30,40,50]})
df.mean()      # average
df.median()    # middle value
df.std()       # standard deviation
df.var()       # variance
df.describe()  # full summary


scikit-learn

1. What is scikit-learn?

Open-source Python library for ML

Built on NumPy, SciPy, matplotlib

Focused on supervised and unsupervised learning

Provides preprocessing, feature selection, model evaluation, and pipelines

2. Core features
Feature	What it does
Supervised Learning	Regression, Classification (LinearRegression, LogisticRegression, RandomForest, SVM)
Unsupervised Learning	Clustering, Dimensionality Reduction (KMeans, PCA)
Model Evaluation	Accuracy, Precision, Recall, F1, MSE, RÂ², cross-validation
Preprocessing	Scaling, Normalization, Encoding, Imputation
Pipelines	Chain preprocessing + model in one object

3. Basic workflow in scikit-learn

Prepare data â†’ X (features), y (target)

Split data â†’ train/test

Preprocess â†’ scale, encode, clean

Train model â†’ fit()

Predict â†’ predict()

Evaluate â†’ metrics

4. Example: Linear Regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import numpy as np

# Sample data
X = np.array([[1],[2],[3],[4]])
y = np.array([2,4,6,8])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)

5. Example: Logistic Regression (classification)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample data
X = np.array([[1],[2],[3],[4],[5]])
y = np.array([0,0,0,1,1])

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print("Accuracy:", accuracy_score(y, y_pred))

6. Preprocessing example
from sklearn.preprocessing import StandardScaler

X = np.array([[1,100],[2,200],[3,300]])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)


StandardScaler â†’ mean=0, std=1

MinMaxScaler â†’ scale between 0-1

ML models love scaled data.

7. Train/Test split & cross-validation
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
scores = cross_val_score(model, X, y, cv=5)
print("CV Scores:", scores)


Cross-validation â†’ avoids overfitting

cv=5 â†’ 5 folds

8. Pipelines

Combine preprocessing + model:

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC())
])

pipeline.fit(X_train, y_train)

Scikit-learn ML Workflow (Step by Step)

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Raw Data    â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Data Cleaning &  â”‚
      â”‚  Preprocessing   â”‚
      â”‚ - Handle NaN     â”‚
      â”‚ - Encode Categorical â”‚
      â”‚ - Feature Scaling â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Train/Test Split â”‚
      â”‚  (or CV folds)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Model Choice   â”‚
      â”‚ - Regression     â”‚
      â”‚ - Classification â”‚
      â”‚ - Clustering     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Model Training â”‚
      â”‚  model.fit()     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Model Prediction â”‚
      â”‚  model.predict()   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Model Evaluation â”‚
      â”‚ - Regression: MAE, MSE, RMSE, RÂ² â”‚
      â”‚ - Classification: Accuracy, F1, Precision, Recall â”‚
      â”‚ - Cross-validation scores           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Hyperparameter   â”‚
      â”‚  Tuning / Grid   â”‚
      â”‚ - GridSearchCV   â”‚
      â”‚ - RandomizedSearchCV â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Final Model      â”‚
      â”‚ - Save using joblib or pickle â”‚
      â”‚ - Deploy / Predict New Data  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
